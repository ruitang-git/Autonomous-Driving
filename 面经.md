# 传统机器学习
## 1. 数据种类
数据主要有以下几种类型(结构化数据和非结构化数据)：
- 表格数据
- 图像数据
- 文本数据
- 时序数据

![alt text](image-108.png)
### 表格数据

表格数据结构化、特征类型复杂，缺乏空间或时序结构，极适合用以树模型为核心的 XGBoost 来建模。

![alt text](Snipaste_2025-06-07_11-30-52.png)

### 常见任务
分类，回归，排序（搜广推），异常检测，特征选择
### 召回率和精确率[1](https://zhuanlan.zhihu.com/p/622493332)
![alt text](image-109.png)
精确率：为了不错报，保证预测的都是对的
召回率：为了不漏报
应用场景：人脸识别支付适合使用精确率，地震预测适合使用召回率

## 2. 算法

### KNN [:a:](https://blog.csdn.net/m0_74405427/article/details/133714384?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522d04bc8b0f59bf0bd7ea62606c9eeeee4%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=d04bc8b0f59bf0bd7ea62606c9eeeee4&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-133714384-null-null.142^v102^control&utm_term=knn&spm=1018.2226.3001.4187)
![alt text](image-141.png)
- 主要调参对象：K值的选取和点距离的计算
- K如何选择：通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。通过交叉验证计算方差后你大致会得到下面这样的图：
![alt text](image-142.png)
- 可用于聚类和回归
- 缺点在于极其消耗内存，因为要存储所有训练集的数据
### 聚类（引申出流形的工作）
#### 2.1 K-Means
![alt text](image-143.png)
![alt text](image-144.png)
![alt text](image-145.png)
k-means++
![alt text](image-146.png)
#### 集成学习
与MoE相似；但MoE更偏重于不同专家处理不同的任务，而集成学习类似于民主投票。
![alt text](image-110.png)
- Bagging
Bagging 特别适合高方差低偏差的模型（如决策树、KNN），因为这些模型对训练数据很敏感（不稳定）。
Bagging 的 bootstrap 采样创造了多样性，让不稳定模型的偏差维持不变，同时降低方差 → 提高泛化能力。
#### AdaBoost, GBDT, XGBoost
![alt text](image-111.png)

## 机器学习汇总
### 按照任务类型
![alt text](image-112.png)
### 按照应用场景
![alt text](image-113.png)

---

# Transformer
> 1. Transformer提出的背景，解决了什么问题？

1. 时间片 t 的计算依赖 t−1 时刻的计算结果，这样限制了模型的并行能力；
2. 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象，LSTM依旧无能为力。
Transformer的提出解决了上面两个问题：
- 使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量。
-  其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。
> 2. Transformer结构以及不同架构的使用场景

![alt text](image-73.png)
- encoder-only
理解类任务，例如文本分类，命名实体识别
模型代表：bert
- decoder-only
生成类任务，例如对话，代码生成
模型代表：chatgpt，llama
- encoder+decoder
seq2seq任务，例如翻译
模型代表：T5（Text-To-Text Transfer Transformer）
> 3. Transformer vs. CNN

1. CNN学习到的feature有平移不变性，包括scale不变性和distortion不变性,Transformer不具备，所以将Transformer运用到目标检测问题上会出现一些问题。
2. 需要建模长距离依赖时CNN不适用。

> 4. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？

使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。
> 5. 为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解

这取决于softmax函数的特性，如果softmax内计算的数数量级太大，会输出近似one-hot编码的形式，导致梯度消失的问题，所以需要scale

那么至于为什么需要用维度开根号，假设向量q，k满足各分量独立同分布，均值为0，方差为1，那么qk点积均值为0，方差为dk，从统计学计算，若果让qk点积的方差控制在1，需要将其除以dk的平方根，是的softmax更加平滑

> 6. 为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？

embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。

> 7. 位置编码需要满足什么条件

- 每个时间步应该为独一无二的，确定的
- 不同长度的句子，任何两个时间步的距离应该保持一致
- 容易泛化到更长的句子，且应该是有界的

> 8. 有哪些位置编码
- 正余弦
我们选择正弦曲线函数，因为我们假设它能让模型很容易地学习关注相对位置，因为对于任何固定的偏移量 k， P E pos+k 可以表示成 P E pos 的线性函数
![alt text](image-232.png)
- 可学习embedding BERT
- 相对位置编码RPE

> 9. 为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？

LN：针对每个样本序列进行Norm，没有样本间的依赖。对一个序列的不同特征维度进行Norm

CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。

> 10. 简答讲一下BatchNorm技术，以及它的优缺点。

优点：

1. 可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的收敛速度。
2. 缓解了梯度饱和问题，加快收敛。

缺点：

1. batch_size较小的时候，效果差。
2. 在RNN中效果比较差。

> 11. 简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？

ReLU
Relu优点：（1）relu函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象。而对于sigmod函数，在正负饱和区的梯度都接近于0，可能会导致梯度消失现象。（2）Relu函数的导数计算更快，所以使用梯度下降时比Sigmod收敛起来要快很多。

Relu缺点：Relu死亡问题。当 x 是小于 0 的时候，那么从此所以流过这个神经元的梯度将都变成 0；这个时候这个 ReLU 单元在训练中将死亡（也就是参数无法更新），这也导致了数据多样化的丢失（因为数据一旦使得梯度为 0，也就说明这些数据已不起作用）。

> 12. Transformer训练的时候学习率是如何设定的？

使用较小的学习率进行warmup，等loss下降到一定程度后，再恢复回常规学习率。
![alt text](image-74.png)

> 13. Transformer训练的Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？

第一次：(1) embedding 的 output
第二次：(2) attention softmax(q*k/sart(d)) 的输出
第三次：(3) attention 层的输出
第四次：(4) ffn 的输出
![alt text](image-75.png)
Dropout可以防止过拟合，某次训练时，随机让某些节点失活，输出为0且不更新权重，通常设置一个参数P，每个输出节点以概率P置0，所以大约每次使用了(1-P)比例的输出。
测试时，去掉Dropout层，将所有输出利用，但是需要对齐尺度，即缩小输出比例。R=R *(1-P)
特别的， 为了使用方便，我们不在测试时再缩小输出，而在训练时直接将输出放大1/(1-p)倍。
作用：Dropout达到了Vote的作用，减少神经元之间复杂的共适应性，可以比较有效地减轻过拟合，一定程度上达到了正则化的效果。

> 14. 怎么选取key的维度

![alt text](image-76.png)

> 15. Transformer的 Q K 可以用别的方法代替吗

可以，只要是计算距离的方法就行，比如说余弦距离

> 16. ViT（Vision Transformer）模型的结构和特点

![alt text](image-77.png)

在NLP领域中的任务如机器翻译，通常是将一个句子的各个词组成一个序列输入到Linear Projection后变成一个个token，然后再输入到Transformer的各个模块，每个词是一个完整的语义信息的单位。但是图片是由一个个像素组成的，如果使用类似于NLP的方法，将像素排列成一个队列输入，对于一张224×224的常规图片，序列长度就为2242=50176，已经远远超过了Transformer可以承载的序列长度；因此，基于此问题，本文提出了一种将图片分块的策略，即将原始图像分成一个个小的patch，这里假设每个patch的大小是16×16，则patch的个数就为2242/162=196，即现在的序列长度。这样一来，序列长度就在Transformer的承载范围之内了。对于更大的图片例如312×312，可以通过改变patch的大小调整序列长度，从而使更大的图像也能够应用于Transformer中。

---
# CNN
- CNN模型有哪些？
VGG，MobileNet，ResNet
- 普通卷积、分组卷积、深度可分离卷积的区别
![alt text](image-86.png)
![alt text](Snipaste_2025-06-04_23-48-13.png)
![alt text](image-87.png)
- MACs和FLOPs
![alt text](image-88.png)
硬件实现使用mac而论文使用flops，通常1 MAC ≈ 2 FLOPs
![alt text](image-89.png)
![alt text](image-90.png)
- Pytorch 的 conv2d 函数的参数有哪些？
![alt text](image-91.png)
![alt text](image-92.png)
![alt text](image-93.png)
![alt text](image-94.png)
- Pytorch 的 DataLoader 原理
PyTorch 的 `DataLoader` 是一个强大的工具，用于 高效地批量加载数据、支持多线程加速、自动打乱、动态批处理等功能。它的底层原理围绕着 `Dataset` + `Sampler` + 多进程/线程协同加载机制(这个设计是为了 不让训练卡在 I/O 或 CPU 数据预处理阶段。)展开。
![alt text](image-97.png)
![alt text](image-98.png)
![alt text](image-99.png)
![alt text](image-100.png)

- 正则化，系数 λ 如何取值？
![alt text](image-95.png)
![alt text](image-96.png)
- 详细描述下你知道的轻量级网络：MobileNetV1、ShuffleNetv1-v2
![alt text](image-101.png)
![alt text](image-102.png)
![alt text](image-103.png)
![alt text](image-104.png)
![alt text](image-105.png)
![alt text](Snipaste_2025-06-05_00-27-39.png)
![alt text](image-106.png)
![alt text](image-107.png)
---
# PyTorch
> 如何截断梯度？

![alt text](image-78.png)

> PyTorch 中最常用的几个库及其用途

![alt text](image-80.png)
![alt text](image-81.png)
![alt text](image-82.png)
![alt text](image-83.png)
![alt text](image-84.png)
![alt text](image-85.png)

---
# 部署
> 减小模型内存占用的方法
- 模型剪枝
- 模型蒸馏
- 模型量化
- 模型结构调整

> 模型FLOPs怎么算？

我们假设卷积核的尺寸是K×K，有C个特征图作为输入，每个输出的特征图大小为H×W，输出为M个特征图。
(1)考虑偏置的情况
总的卷积运算计算量（乘法+加法）：
2×H×W×M×C×K×K(加法：H×W×M×C×K×K)
(2)不考虑偏置的情况
H×W×M×(2×C×K×K−1)

---
**1. Q:行为预测和轨迹预测**
A：是耦合的，但不一样。行为一般指目标车未来会采取什么动作，变道停车超车加速左右转直行等等。轨迹的话就是具体的具有时间信息的未来可能的位置点
**2. Q.补充对数据集的了解Argoverse***
Argoverse分为argo1和argo2两个数据集；其中轨迹预测的数据每个场景存储为一个.parquet数据，包含1. 场景元信息 2.轨迹数据 3. 高精地图
轨迹预测目标为给定：过去5秒、地图上下文；预测：未来6秒的主要交通参与者的轨迹
![alt text](1746955044359.png)
仅argo2支持多目标预测！
**3. Q. 使用相对坐标和绝对坐标**
- 泛化性的提升
  - 平移不变性需求：在实际的交通场景中，场景整体的平移不应该影响智能体之间的相互运动关系和模型的预测结果。相对坐标通过刻画各实体之间的相对位置关系，天然地满足平移不变性。
  - 模型泛化能力增强：相对坐标更能体现智能体之间的内在关系和相互作用，使模型学习到的特征更具通用性。无论场景在何处发生，只要智能体之间的相对关系相似，模型就能利用之前学习到的知识进行准确预测。而绝对坐标可能会使模型过度依赖于特定场景的绝对位置信息，导致在不同场景下的泛化能力下降。
- 计算效率的提高
  - 数据维度降低：绝对坐标需要以全局原点为参照来表示每个智能体的位置，通常需要两个或三个维度（二维或三维空间）来描述。
  - 减少坐标转换计算：在处理多智能体运动数据时，如果使用绝对坐标，当需要计算智能体之间的相对位置或进行一些基于相对关系的计算时，往往需要先将各个智能体的绝对坐标进行转换，计算出它们之间的相对位置关系。而使用相对坐标则可以直接获取这些相对信息，避免了繁琐的坐标转换计算步骤。
  - 
**4. Q. 车辆预测vs行人预测的异同**

**5. Q. 轨迹预测和高清地图的关系，影响有多大**
**6. Q. uniad端到端模型中预测部分存在什么缺陷**
1.做的是marginal prediction不是joint prediction；2. prediction和planning是分开来做的，没有显式考虑ego和周围agent的交互博弈；3.用的是scene-centric representation，没有考虑对称性
**7. Q. 轨迹预测的多种建模范式**
- agent-centric以agent为中心
- scene-centric以场景为中心
- goal-driven以目标为导向
- Map-Centric以地图/环境为中心

**8. Q. 轨迹预测：行人vs预测**
和人相比，车的运动特征很特别的一点就是轨迹是受到动力学限制的，而且车只能在马路上行驶，所以也是受到诸如车道线和路口形状等的限制。

正是由于这些机动车的行为特征，我们的策略就是先对车辆的意图进行预测，这里的意图指向比较广泛，车辆会选择哪条车道，会不会变道的意图，或者说车辆进入路口后，是会左转、还是直行等等。一旦我们有了车辆的意图，我们再结合车的运动学原理，便可以描画出更加具体的轨迹。而且，我们对常规道路和交通路口的车辆，也会运用不一样的预测模型进行处理。

同时只给行人的中心点坐标是没法预测行人轨迹的。行人非刚体，随机性太大了，至少要有点行人的姿态信息才有可能预测得好一些。

![alt text](1747068405054.png)
![alt text](1747068454983.png)

**9. Q. 经典数据集**
![alt text](image-1.png)
行人：
- ETH/UCY
- Stanford Drone Dataset 
- TrajNet++
车辆：
- nuScenes
- Argoverse
- Waymo Open Motion Dataset
**关键差异总结**
1. 行人数据集：
标注社交关系（如群体行走）。
需处理遮挡、突然停顿（如ETH中行人驻足聊天）。
1. 车辆数据集：
标注车道中心线、交通灯状态（如Argoverse的红绿灯时序）。
需匹配高精地图（如nuScenes的HD Map API）。
**挑战差异**
1. 行人预测难点：
**短期**突发行为（如奔跑）、意图歧义（走向商店or车站？）。
1. 车辆预测难点：
**长时**多模态预测（如路口左转vs直行）、V2X协同一致性。

**10. Q. 什么是PyG**
PyG (PyTorch Geometric) 是一个基于 PyTorch 的库，专门用于图神经网络 (Graph Neural Networks, GNNs) 的开发。

它提供了各种图数据结构的支持，以及常见的 GNN 模型（如 GCN、GAT、GraphSAGE 等）
**11. Q. 什么是自行车模型**

可与数据驱动方法结合，提供
- 初始猜测：为机器学习模型（如LSTM、Transformer）提供物理合理的轨迹先验。
- 后处理优化：修正数据驱动模型的输出，使其符合运动学约束。
- 混合方法：如用神经网络预测控制输入（转向角、速度），再通过自行车模型生成轨迹。

**局限性**
1. 忽略动力学效应（低速假设失效）
问题：自行车模型基于纯滚动假设（无轮胎滑移），仅适用于低速场景（通常 < 5 m/s）
2. 无法处理复杂地形或路面条件
3. 未考虑车辆特异性参数


**12. Q. 轨迹预测vs轨迹补全**
![alt text](image-2.png)

**13. Q. 单vs多模态轨迹预测**
自动驾驶系统普遍采用多模态轨迹预测的核心原因，正是因为它能为后续的路径规划、决策制定提供更丰富的信息输入，从而提升系统的安全性和适应性。


**14. Q. BEV vs SLAM**
两者在自动驾驶中定位的区别？

**15. Q. 内参(Instrinsics)和外参(Extrinsics)**
相机有两个最基础的数据：内参(Instrinsics)和外参(Extrinsics)，内参主要描述的是相机的CCD/CMOS感光片尺寸/分辨率以及光学镜头的系数，外参主要描述的是相机在世界坐标系下的摆放位置和朝向角度。

**16. 卡尔曼滤波Q和R怎么调**

**17. 轨迹预测模型**
- 基础模型
  - HOME
  - VectorNet
  - LaneGCN
- 进阶模型
  - TNT（基于VectorNet的改进）
  - GOHOME/THOMAS（基于LaneGCN的改进）
  - HiVT

---
# LINUX八股
## 常用指令
- `top`实时监控进程
- `ps -ef | grep java`查看所有进程（筛选java）
- `ps`显示当前用户进程 `ps -e`显示所有用户进程 `ps -f`显示进程详细信息
- `netstat` 查看网络链接
- ls：列出目录内容。ls -l显示详细信息，ls -a显示隐藏文件
- cat	快速查看文件内容	vim 编辑文件
- `kill` `kill -9 PID`强制关闭
- `ps aux --sort=-%cpu | head -5`列出cpu排行前5的进程
- 查看内存 `free -h`-h为-human，单位自动切换，更友好
- chmod 的参数讲一下？
  chmod 格式：chmod xxx file
  ![alt text](image-147.png)
  ![alt text](image-148.png)
  ![alt text](image-149.png)
  ![alt text](image-150.png)
  ![alt text](image-151.png)
  `usermod`
- 网络管理的命令有哪些？
  wget：从网络上下载文件
  ping
  netstat显示网络连接、路由表和网络接口信息
  ifconfig显示网络接口的配置信息
- 如何查看8080端口的连接数？
  `netstat -an | grep ':8080' | grep 'tcp' | wc -l`wc -l统计行数
- 压缩和解压缩的命令有哪些？
  tar仅打包， gzip仅压缩（单个文件），zip（打包并压缩）
  Linux 上常用组合是：先用 tar 打包，再用 gzip 压缩，生成 .tar.gz 文件
  `tar czf archive.tar.gz 目录名或文件名`c:create,新建归档文件；z：通过gzip压缩；f：指定归档文件名
  `tar xzf archive.tar.gz -C 目标目录/`
- 查找文件的命令有哪些？
  `find /directory/ -name filename`
  `find /home -name "*.txt"`查找txt文件
- 用户和用户组
  linux中包括普通用户和超级用户（root）
  - 创建新的用户组
  `sudo groupadd developers`sudo: superuser do
  - 创建新的用户
  `sudo useradd -m -g developers john` -m：表示创建用户的同时创建用户的主目录（通常在/home/username）。-g：指定用户的初始用户组
- Linux系统中如何查看系统的磁盘使用情况？
  可以使用df命令来查看系统的磁盘使用情况
- CPU 负载和 CPU 利用率的区别是什么？
  cpu负载表示当前系统正在运行的和处于等待运行的进程数之和。也指的是处于可运行状态和不可中断状态的平均进程数；CPU利用率指的是当前正在运行的进程实时占用CPU的百分比
  - CPU 负载很高，利用率却很低该怎么办？
  CPU 负载很高，利用率却很低，说明处于等待状态的任务很多，负载越高，代表可能很多僵死的进程。通常这种情况是IO密集型的任务，大量请求在请求相同的IO，导致任务队列堆积。
  同样，可以先通过top命令观察(截图只是示意，不代表真实情况)，假设发现现在确实是高负载低使用率。
  然后，再通过命令ps -axjf查看是否存在状态为D+状态的进程，这个状态指的就是不可中断的睡眠状态的进程。处于这个状态的进程无法终止，也无法自行退出，只能通过恢复其依赖的资源或者重启系统来解决。

  - CPU 负载很低，利用率却很高该怎么办？
  这表示 CPU 的任务并不多，但是任务执行的时间很长，通常是计算密集型任务，生成了大量耗时短的计算任务。
  直接 top 命令找到使用率最高的任务，定位到去看看就行了。如果代码没有问题，那么过段时间CPU使用率就会下降的。
- chmod和chown的区别
![alt text](image-152.png)
- 如何保证linux系统的更新
  `sudo apt update         # 更新软件包列表`
  `sudo apt upgrade        # 安装可用升级包（不移除软件）`
  `sudo apt full-upgrade   # 安装所有更新（可能会移除部分包）`
- 如何监控系统性能并诊断问题
- 如何创建linux开发环境，服务器上怎么运行代码？
  联合pycharm和vscode

---
# AI Infra
- AI编译器
  - 什么是AI编译器
  AI 编译器（AI Compiler）是为加速和优化机器学习模型在各种硬件平台（如 CPU、GPU、TPU、NPU 等）上的执行而设计的“编译工具链”。它的作用是将高层的模型描述（如 PyTorch/TensorFlow）转换为高效的底层代码，使模型运行更快、更省内存、更适配硬件。
  - 编译器结构
    ![alt text](image-157.png)
  - AI编译器的常用技术
  ![alt text](image-155.png)
  - 常见的编译器有哪些
  ![alt text](image-156.png)
  - 是否要从0开发编译器
  ![alt text](image-158.png)
  ![alt text](image-159.png)
  - ONNX
  .pt无法在非pytorch后端运行，需要转换为.onnx。ONNX 相当于连接训练框架和部署硬件之间的桥梁，是描述计算图的一种标准格式，通过定义一组与环境和平台无关的标准格式，使AI模型可以在不同框架和环境下交互使用。
  通过用ONNX表示更容易部署的静态图。硬件和软件厂商都只需要基于ONNX标准优化模型性能，让所有兼容ONNX的厂商获益。
  ![alt text](image-160.png)
    - 如何使用pytorch导出onnx：
      `torch.onnx.export`
      ![alt text](image-162.png)
      `torch.jit.trace`需要传入一个实例
    - onnx模型校验
    `onnx.checker.check_model()`
    - onnx模型可视化
    `pip install netron`
    `netron model.onnx  # 会自动在浏览器中打开`
    - onnxruntime
    ![alt text](image-163.png)
    ![alt text](image-164.png)
    - cpu推理时间大概变为1/3
    - 推理流程
    $PyTorch \rightarrow ONNX \rightarrow ONNX Runtime$
- 推理引擎
  - 输入：通常是一个已经编译好的、硬件可执行的模型文件（例如 TensorRT engine、ONNX Runtime session、OpenVINO IR等）
  - 功能：
    - 管理模型加载
    - 调度硬件资源（CPU/GPU/加速卡）
    - 高效执行前向计算（如卷积、矩阵乘法、激活函数等）
    - 优化执行性能（融合算子、内存复用、并行计算等）
  - 有哪些：
    - TensorRT
    - ONNX Runtime
    - OpenVINO
  - vs. AI编译器
  ![alt text](image-161.png)
- 中间态 DAG计算图，基于计算图的编译系统，编译组件会将python编译为计算图；计算图解释器
- 性能调优
  - 模型量化理论：追求高B，低Q
    - kv cache quantization
  量化模型平均恢复率均超过99%和96%以上
  DPD的量化位数为多少
  - context长度理论
  - GPU offload理论
  ![alt text](image-195.png)
  - CPU thread
  - keep model in memory
  - temperation
    - 0-1;越大模型创造力越高
- AI infra
  ![alt text](image-153.png)
  ![alt text](image-196.png)
- 计算起到什么作用？
- 计算图优化是通过编译器实现的
  ![alt text](image-154.png)
- 计算图切分与多设备执行
  可以抽象成一个优化问题，运筹学范畴

---
# C++
- 安装
  - mingw
- 命名空间，不同命名空间同样的函数不冲突
  通常在文件开头加`using namespace std;`,即可忽略`std::`
  ![alt text](image-165.png)
- 输出
  ![alt text](image-166.png)
- 变量
  ![alt text](image-167.png)
- 常量
  ![alt text](image-168.png)
- 输入
  ![alt text](image-169.png)
- 运算符
  ![alt text](image-170.png)
  ![alt text](image-171.png)
  ![alt text](image-172.png)
- 条件
  ![alt text](image-173.png)
  ![alt text](image-174.png)
- 循环
  ![alt text](image-175.png)
- 数组
  ![alt text](image-176.png)
- 引用and指针
  ![alt text](image-177.png)
  ![alt text](image-178.png)
  ![alt text](image-179.png)
- 函数
  提前做函数声明
  ![alt text](image-180.png)
  ![alt text](image-181.png)
  ![alt text](image-182.png)
  - 重载
  ![alt text](image-183.png)
- 类
  ![alt text](image-184.png)
  构造函数，相当于python的__init__
  ![alt text](image-185.png)
  通过公有方法访问私有属性
  ![alt text](image-186.png)
  继承（可以继承多个类）
  ![alt text](image-187.png)
  ![alt text](image-188.png)
  ![alt text](image-189.png)
- 多态
  多态的核心思想：同一操作作用于不同的对象，可以有不同的解释，产生不同的结果
  多态的分类：
    - 编译时多态（静态多态）
      - 函数重载
      - 运算符重载
    - 运行时多态（动态多态）
      - 通过基类的指针或引用调用派生类的重写函数（虚函数）
      ![alt text](image-194.png)

- 文件
  ![alt text](image-191.png)
  ![alt text](image-192.png)
- 异常
  ![alt text](image-193.png)
  所有异常捕获`catch (...)`

---
# 模型推理
- deepseek了解吗？用了什么新的技术？对目前大模型动向的理解
- 对attention的理解
- 为什么要用layernorm
  模型训练的稳定性好（不做容易梯度爆炸或消失），收敛快，不依赖weight的初始化
- 大模型基础问题
- 矩阵乘法思路
  有多种优化的思路。只了解分块矩阵加速的介绍。
  ![alt text](image-208.png)
  ![alt text](image-209.png)
  ![alt text](image-210.png)
- 流水线，分支预测，乱序执行
  这是处理器（CPU）设计中提高性能的三大核心技术：
  1. 流水线（Pipeline）
    🧠 本质：将一个指令的执行流程分解成多个阶段，多个指令可以在不同阶段并行进行。
  2. 分支预测
   🧠 本质：遇到 if/else 或循环跳转时，CPU预测下一步跳到哪，提前开始加载和执行指令。现代 CPU 分支预测命中率高达 95%+，极大减少流水线空转（bubble）损耗
  3. 乱序执行（Out-of-Order Execution）
  🧠 本质：如果某些指令还不能执行（比如数据没到），就跳过它，先执行后面的指令，不等死。
  ![alt text](image-211.png)
- pytorch各种相关
- c++八股
- tvm的理解
  TVM（Tensor Virtual Machine）是一个用于机器学习模型编译与优化的开源深度学习编译器堆栈。它的目标是在多种硬件后端（CPU、GPU、AI 加速器）上实现高性能、跨平台的推理部署。
  ![alt text](image-212.png)
  ![alt text](image-213.png)
- 谈谈网络量化
  目的：降低功耗，提升运行速度，降低内存，减小芯片面积
  类型3种：QAT（在预训练或微调期间应用量化，BP需要用到STE），PTQ
  - 什么是weight-only
    只对权重进行量化，容易的多。因为权重训练后是静态的。
  - AWQ
  为weight-only方法，且为PTQ。对所有通道采用同样的bit数进行压缩，但选取不同的scale。
- 思考反问的问题
- 如何估算模型所需要的RAM？
  主要分为三部分：
    - 模型参数
     对于 fp32，LLaMA-6B 需要 6B*4 bytes = 24GB内存
    - 梯度信息
    等于参数量*每个梯度参数所需内存。
    - 优化器参数
    对于常用的 AdamW 来说，需要储存两倍的模型参数（用来储存一阶和二阶momentum） fp32 的 LLaMA-6B，AdamW 需要 6B*8 bytes = 48 GB
- 主流的推理引擎
  ![alt text](image-198.png)
- 推理优化技术总结
  推理优化涉及分布式优化（通信层面），低比特量化（降低显存），算子优化（提升算子的优化效率），访存优化（减少GPU对HBM-high bandwidth memory的访问），服务并发优化，其他新技术6个方面
  ![alt text](image-206.png)
  - 服务并发优化
    传统批次处理通过将多个请求合并为一个大的batch送进模型，等待请求的过程不适合低时延业务
    - continuous batching
      流式聚合就是利用流水线的思想，
  - 显存优化
    - KV Cache
    核心思想：
    ![alt text](image-199.png)
    缺陷：
    ![alt text](image-200.png)
    - page attention
    每个页面都有生命周期管理，当到期时，则会释放
    ![alt text](image-201.png)
    - flashattention
      - 写出self attention公式
      ![alt text](image-202.png)
      - softmax在工程中可能会出现数值溢出，用以下方式可以解决
      ![alt text](image-203.png)
      - safe softmax需要多次访存，怎么解决
      ![alt text](image-204.png)
      ![alt text](image-205.png)
    - MQA/GQA
      目标是解决传统多头注意力每个head都需要一套KV的问题，显存占用过高。降低key/value数量，节省内存和计算，同时保持模型效果。
      - MQA
      每个head共享KV，只有Q是多头的。与KV cache天然适配。
      - GQA
      将多个 Query Head 分组，每组有多个 Query heads，但共用一组 K 和 V
      - 为什么不共享Q呢？
      1. 从注意力机制的物理意义上应该是多个Q
      2. Key/Value 是「只读缓存」：性能瓶颈就来自它！在推理时，Key/Value 会被缓存（KV Cache）
  - 低比特量化
  - 分布式优化
    - 调度优化
    e.g., Continuous Batching：在 动态到达的请求流 中，为每个请求分配批处理时间和资源，使得在满足延迟、显存约束的前提下，最大化 GPU 的吞吐量和利用率。可以抽象为一个MILP问题。
    ![alt text](image-219.png)
    ![alt text](image-221.png)
    - 资源分配与容量规划
    e.g., 背包问题（Knapsack）:如何在有限 GPU 显存下布置模型副本。利用动态规划求解
    ![alt text](image-222.png)
    - 图优化(模型并行)：
    e.g., 图划分：计算图的划分与调度（Graph Partitioning）
    ![alt text](image-223.png)
    ![alt text](image-224.png)
  - 算子优化
    - 算子融合
      ![alt text](image-207.png)
      GEMM：广义矩阵乘法
      优势：1避免重复读写操作2.量化下减小中间变量的精度损失
      经典：conv+bn; conv+relu
  - 其他
    - 投机采样：提前猜多个token并进行下一步计算


---
- BN vs. LN
- 图神经网络在轨迹预测中的应用（异构图）， langGCN，vectornet， densetnt
- multipath++

---
# 简历问答
## 项目一 卡车排编队
1. 简单介绍一下项目的具体工作
- 卡车排编队问题的背景和意义：合作博弈，规划任务，最优化
- 研究现状：
  - 路径规划
  - 编队队形控制：PID， MPC
  - 编队动态调整
    - 上层决策层：规划编队调整策略
    - 下层执行层：控制器，调整速度完成并入...
  - 研究任务：
  **同时考虑路径规划和编队调整控制，完成规划**
- 主要挑战
  - 车辆兼容性，与自动驾驶不同，车间通信在该场景下考虑更多，通信中断会导致编队解体
  - 对跟车精度要求高，控制器要求高
2. 怎么建模
- 首先明确优化目标是什么，优化参数是什么？
  - 优化目标：油耗（和路线上的卡车数量有关，卡车排越多，油耗越省。直觉上看就是尽可能让更多的车共享一个车道），额外加惩罚项（需要在规定的时间内到达）
  - 优化参数：每个卡车在每个时刻应该位于那条道路
  - 对道路建模2D（Erdos-renyi静态图，但要保证图是strict-conncected）
  - 得到所有优化参数：$K*N*N*T$
- 进一步对constraint建模
  - 保证路径合法（类似电路的基尔霍夫定理）
  - 优化参数为0或1
- 转化为MILP，引入辅助变量
  例如max函数，引入辅助变量，最小化辅助变量，同时约束项中添加两条
1. 怎么求解？
- 中心化
使用cvx的gruobi求解器 `variable x(2) binary`
融合了分支定界，gomory割平面
```python
## cvx书写规范
#1. 等式约束：左右两边均为 仿射表达式（线性
A*x == b       % 合法
norm(x) == 1   % 非法（非线性等式）
#2. 不等式约束：需满足 凸性（如 <= 右侧为凸函数，>= 右侧为凹函数）。
norm(A*x - b) <= 1      % 合法（凸约束）
x'*Q*x <= 1             % 合法（若Q为正定）
x(1)*x(2) >= 1          % 非法（非凸）

## e.g.,
sum(abs(x)) 不是 CVX 直接支持的凸函数形式：

虽然数学上 ( \sum |x_i| = |x|_1 \，但 CVX 要求显式使用 norm(x, 1))

```
4. 为什么要实现分布化？
   1. 去中心化更鲁棒，中心节点若出现故障，则容易引发全局问题；高可靠性和容错性
   2. 去中心化可以减少对中心节点资源的依赖，同时更好的进行并行化，降低整体解算时间（特别是网络规模变大，分布式解算时间影响小）
   3. 去中心化在通信上面节约时间且无需特殊设计

5. 分布式割平面法
   1. 解算模块
      1. gomory割；
      ![alt text](image-115.png)
      ![alt text](image-116.png)
   2. 通信模块
      1. 交换基
      求解 LP 时，一个最优顶点解总是由精确等于变量个数 d 的线性无关约束组成的集合决定的，这个约束集合就叫 basis（线性规划中的“基”）
      2. ![alt text](image-114.png)以$\epsilon$尺度减小
      3. 收敛条件本地 basis 在连续 $T=2{d_G}+1$ 步中保持不变;为什么是这个数：所有约束信息最多需要 ${d_G}$ 步传播，收敛的“回波确认”最多再传播回来 ${d_G}$ 步，加一步缓冲
6. 在参数设计上要注意什么
   1. 图的直径和起点终点选择
   2. 保证图是strict connected 
   3. 为保证一定有解，使用big-M method
7. 求解使用的模块
`python gurobi(callback)`

## 项目二 分布式粒子滤波

1. 应用场景
   多传感器系统或多智能体系统中实现状态估计的技术
   1. 多车辆/无人机协同定位
   2. 大规模无线传感器网络中的目标跟踪（用分布式传感器（如麦克风阵列、雷达节点）跟踪移动目标（如入侵者、野生动物、火源）
   ![alt text](image-118.png)
2. 研究现状
   1. 什么是状态估计？
   状态估计（State Estimation）是控制理论、机器人、导航、信号处理等领域的核心问题之一，其目标是根据部分可测量的输出（观测值）估计系统内部的不可测状态变量。这一问题广泛应用于飞行器导航、SLAM、自动驾驶、工业过程控制、电力系统等。
   2. 经典算法
   ![alt text](image-117.png)
   卡尔曼滤波（卡尔曼增益）
   ![alt text](image-225.png)
   3. FastSLAM, 粒子滤波在slam中的应用
   4. 传统滤波算法对比深度学习，KalmanNet使用深度学习网络学习卡尔曼增益（避免了对过程/观测噪声协方差的精确建模需求）
   - 传统滤波显示建模，可充分利用先验信息
   - 同时传统滤波对先验的依赖过强也是缺点
   - 传统滤波对数据量要求小
   - 序列建模能力传统有限，深度学习可以做到长期依赖
   - 传统滤波可解释性强，复杂度小，可嵌入系统
3. 粒子滤波
   1. 重要性采样
   2. 序贯重要性采样
   3. 粒子退化
   4. SIR：如何选择采样函数，直接使用$p(x_n|x_{n-1})$
   5. Graph Laplacian如何选择k的大小：k过小：拉普拉斯矩阵有多个零特征值，频率分布不连续。k过大：这使得L趋近于一个“常数-模式”矩阵，其特征值大部分接近相同
   6. 不同的分布式传输协议：gossip，flooding
   7. GMM
      1. 如何确定GM数量：$AIC=2k−2logL$，越小越好
      2. ```from sklearn.mixture import GaussianMixture```
      3. EM algorithm
         1. 目标：最大化的是整个数据集的对数似然
         2. 初始化：KMeans 聚类中心作为$\mu$,单位矩阵或数据协方差初始化方差，$\pi_k=1/K$
         3. Expectaion
         计算每个样本属于每个成分的概率
         4. Maxmization
          更新权重，均值，方差
         5. 你不知道每个数据点属于哪个“隐类”（比如哪一个高斯），所以在 E 步你“猜测”（根据当前参数算后验），然后在 M 步“根据这个猜测”更新参数。再根据新参数重新猜测... 循环直到收敛。
      4. consensus
         1. 使用metropolis进行汇聚
          ![alt text](image-119.png)
         2. 若干个高斯分布相乘很难得到融合后的解析解：通过IS解决
4. 高斯过程重采样
![alt text](image-120.png) 
   1. 高斯过程：定义
   2. 可以用于回归，分类，降维
   3. 协方差：基于 radial basis function(RBF)
   4. ![alt text](image-121.png)
   5. 常用场景
      1. 函数不可微/无法估计
      2. 样本数量有限
      3. 平衡exploration and exploitation
       ![alt text](image-123.png)
       ![alt text](image-124.png)
       虽然 exploration 带来轻微 ESS 降低，但显著提升了 RMSE 准确率。
   ![alt text](image-122.png)
5. 在LLM推理中的应用
作者使用粒子滤波（Particle Filtering）方法对大语言模型（LLM）的推理阶段进行改进，解决了现有确定性推理方法（如Beam Search）在早期剪枝时误删潜在正确路径的问题。将LLM推理过程建模为一个状态空间模型（State Space Model），其中LLM是状态转移模型（生成下一个token的机制），PRM（过程奖励模型）是观测模型（判断中间推理步骤好坏）。使用粒子滤波在这个状态空间中进行近似后验采样，而不是像Beam Search那样只保留局部最优路径。每一步维护多个“粒子”（推理路径），根据奖励模型的得分进行加权重采样，从而在高得分路径上“利用”，在低得分路径上“探索”，不会因为单步得分低而彻底丢弃整条可能正确的路径。

## 项目三 模型加速和超参优化
1. Koopman
![alt text](image-125.png)
2. DMD 是 Koopman 的有限维逼近
直观理解：Koopman 是“数学理想模型”——真正告诉你动力系统如何变化（包含所有非线性行为）；
DMD 是“工程工具”——你拍了系统一系列快照，想从这些图像中猜出它是怎么动的。动态模态分解的目标就是计算A的特征值及特征向量，即完成对近似线性动力系统进行模态分解。
A的特征向量提供了模态信息，特征值提供了模态的增长率及频率信息。
![alt text](image-126.png)
3. 工作流程
   ![alt text](image-127.png)
   ![alt text](image-128.png)$\tilde{A}$是DMD矩阵
   ![alt text](image-129.png)
   涉及矩阵求逆，不适合大模型
4. 关键点：
   1. SVD时降维的目的：去噪，在一个高维空间找到低维流形，预测效果会很差，类似于过拟合；**保留小奇异值 = “强迫”模型解释这些噪声**如何选取合适的数量保存（pydmd自动判定）：
   ![alt text](image-227.png)
   2. 快照一般在30左右，太小会导致降维空间太小，无法捕捉主要动态模态
5. 实际过程中会跑飞，怎么解决
   1. 仅用于短期预测（局部线性），有意识地限制预测步长；
   2. 小时间步长可以近似为线性；过小是噪声大，过大非线性；
   ![alt text](image-228.png)
6. 代码 `pyDMD库`
7. 实际上为了保持稳定和加速更多，采用以下策略：前期预测步数很小，后期逐渐增大；DMD for AI training分好几种，采样直接对全量网络参数进行预测。
8. 机制：训练一个神经网络（如 LSTM）来学习更新权重的策略；
**超参寻优**
grid search with SH:每个组内有200个配置，最早停的为100次迭代，最后的为3000次。组合有（3* 4 *3）^16,包括相位和信号项的选择。将近1w次配置寻优。
~1dB的收益
   
![alt text](image-226.png)
## 项目四 AI-DPD(非因果预测任务)
**核心在于量化，剪枝，蒸馏，降低复杂度，加速推理和训练**
- 要点一
  建模输入信号为复数信号；实数和虚数建模不利于频率结构的学习。原因在于输入信号的相位或频率是连续变化的，直接对于IQ信号使用CNN或MLP，会导致频率信息丢失，物理上无意义。

Gate-NN
adaptive tree
k-means
CNN
Informer/CAWformer/ASSA-LSTM-Transformer

![alt text](image-248.png)
### 特征提取
1. 上一代模型的局限性
多层级联模型，第一层为基于经验和OMP筛选的多项式特征，第二层为时间序列上的1D卷积
#### 1. 传统特征提取

#### 2. 基于神经网络
![alt text](image-251.png)
- CNN
  - Convolutional Neural Network for Behavioral Modeling and Predistortion of Wideband Power Amplifiers
  ![alt text](image-249.png)
  - 非线性函数不能使用relu，会引入严重的频谱泄露；relu存在0处不可导（不光滑），x<0时为0（类似削波），带来高频信号。
  - TCN-DPD: Parameter-Efficient Temporal Convolutional Networks for Wideband Digital Predistortion
 ![alt text](image-250.png)
    - 引入dilated conv解决长时依赖问题
    - 1x1 convolution - 增加channel的数量
    - depthwise separable convolution block
    - 将2D conv转为2个1D conv（特征维度上使用1x1 conv，时间维度使用dilated conv）
  **Dilated + Residual + Fully Convolutional = TCNN** 
  - modernTCN**时序分析中保持变量维度很重要，丢弃变量维度会导致性能下降**
    - embedding：Maintaining the Variable Dimension，对单独的variable在时间维度上patch，进行卷积，将patch维度内的信息embed到D维度上
    - DWConv：针对独立的variable和feature，在时间维度上做conv
    - convffn1：针对feature dimension做conv
    - convffn2：针对variable dimension做conv
- Transformer
  - TFT：Temporal fusion transformers for interpretable multi-horizon time series forecasting（2019）
  使用了LSTM+Transformer结合的思路
  - Log-Sparse Transformer：Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting（2019）
  Transformer使用了基于原始序列p2p之间的相关性。本文先使用CNN对原始序列进行特征提取，再使用transformer，这样就能让attention不仅考虑每个点的值，也能考虑每个点的上下文信息，将具有相似形状的区域建立起联系。有时候，单点之间不一定存在相关性。
  - Informer
    - 针对transformer在长序列信号处理时的O(L^2)复杂度，提出了ProbSparse Attention，对于每个q，只对log(L)个key进行计算，只取topu个q，其余的都做平均（attention score具有非常明显的长尾性）
    - 针对多层transformer占内存的行为，使用distill在序列方向降低长度，通常是 1D convolution + max pooling
    - 使用了生成式解码器（the generative style decoder），一次性生成所有的输出
  - Autoformer
    - series decomp
    - auto correlation
  ![alt text](image-252.png)
  - Pyraformer
  - ETC
  - LogTrans
### 时序模型



因为real time和latency，模型没法做复杂  
1. 什么是DPD，作为一个物理系统系统，如何建模，有什么特别的

背景介绍：我们希望中频器件是理想的，对于射频功放，我们希望它能够无失真的把信号放大，但实际上存在非线性失真。并且，功放的效率越高，通常非线性失真越严重。一方面，我们希望尽量让功放的效率尽量高，这样节能。但如果我们不对信号进行校正，首先在接收端我们难以还原出真实的发射信号（从星座图上可以看到信号都混在一起），其次由于是非线性失真，意味在频率域上会有弥散，信号会泄露到相邻信道，这是协议明确禁止的。所以我们会在信号进入功放之前，在数字域上进行预失真处理，理想的系统响应做到和功放的系统响应的逆函数，这样就弥补了。

抽象建模：我们模型的输入就是数字复数信号，491MHz，通过我们的模型得到预失真信号，本质可以抽象为一个时间序列非线性回归问题。可以用一系列深度学习时序回归的模型去做，例如，RNN，LSTM，Transformer等等。

物理特性：
- 由于物理器件的原因，非线性带有“记忆效应”带来建模难度，在低功率区具有很好的线性度
- 仅关注频点附近的失真情况,带外的通过滤波器滤除，可适当降低建模难度，因此在构建建模项人为需要关注阶数，避免做很多带外无谓的建模，浪费资源
- 宽频时：与单频的区别，由于频谱宽，非线性维度变高，模型项变多（交调-跨频段，三次项）；相位响应更复杂，需要花更多资源进行建模

理论模型：
Volterra级数用于描述非线性系统响应的一种数学模型，特别适合建模具有“记忆性”的弱非线性系统：给定范围提供足够的非线性阶次和记忆深度，能够以任意精度对有记忆的非线性系统进行逼近
缺点：参数过多，5阶和记忆长度为8时，参数量达到4320
![alt text](image-130.png)

模型演进：
- 记忆多项式模型：旨在找寻高效的建模项
举例高效的建模项：
![alt text](image-131.png)
![alt text](image-132.png)
可以采用OMP进行特征筛选：![alt text](image-229.png)
- Wiener-Hammerstein模型
模仿PA内部的结构，将模型变为：LTI-NL-LTI的级联结构，分别对记忆和非线性建模
- 对幅度和相位分开建模：具备物理意义
![alt text](image-133.png)![alt text](image-134.png)
- 分立模型
  - abs：幅度非线性核心：![alt text](image-135.png)
  - sl：延迟项：![alt text](image-136.png)
  - sil：交叉延时
- 级联模型：LUT，FIR，||，乘法器...
![alt text](image-137.png)


2. 如何精简模型，技巧
   1. 优化模型结构和参数
   2. 插值，提高分辨率。因为射频是连续信号，预处理是数字信号，分辨率固定。
   3. 宽频场景：建模项$x_1*LUT(|x_1|,|x_2|,|x_3|)LUT(\phi_1,\phi_2,\phi_3)$,对表达式的不同项进行级联或者堆叠建模，但必须保证最后的表达式符合物理，即乘法器，加法器的位置要明确

3. AI如何帮助解决一些传统算法难以解决的问题
   核心矛盾不在于性能如何提升，在于保证计算资源不上涨的情况下如何优化模型结构
  - 当前的模型已经借助了AI中的级联结构，提升了特征的学习能力
  - L1正则化，resnet（性能接近极限，但参数量为上百倍）
  - 还有哪些研究方向：频域DPD（MoE），核函数近似LUT（用多个1Dlut替代高维lut）...
  - CNN帮助特征筛选： 
  CNN参数共享，降低复杂度节省资源；可以提取局部特征；前期模型都是1D FIR，需要完成参数选择
    - 使用两个CNN完成特征项组合：第一级（用于生成特征项）CNN吸收了这个操作，同时为增加复杂度，第二维为经过不同1D FIR的信号。通过这个操作吸收了非线形层。第二层用于完成特征项的组合。并使用多个卷积层；并使用多个模块级联实现FPN的效果
    - 虽然参数量并没怎么增加，但是MAC数大大增加，为了落地还需要继续降低复杂度
  - ![alt text](image-230.png)（深度可分离卷积，空洞卷积，池化）
  - ESN
    - 提出背景：1. 传统 RNN 训练困难（梯度爆炸/消失）2. 全参数更新意味着时间复杂度高，训练时间长
    - 创新：把动态记忆能力（循环网络）固定住，只训练最后的输出层
    - 优势：训练快：只训练输出层；动态建模能力强：Reservoir 提供丰富的动态表示避免梯度问题：不用 BPTT；实现简单：适合硬件部署（如 FPGA）；直接得到闭式解：
  ![alt text](image-231.png)
    若需要在线更新，可使用RLS算法：
    ![alt text](image-140.png)
    - 局限:Reservoir 随机，需调参（如谱半径、稀疏度）；只能处理中短期依赖（长依赖处理不如 LSTM）；任务结构不明确时，Reservoir 可能效率不高
    - 初始化reservoir：谱半径（矩阵的最大特征值模）<1(采用后归一法)；保证Echo State Property（ESP），在足够长时间后，Reservoir 的状态只依赖于输入序列，而不依赖于初始状态。
    ![alt text](image-138.png)
    - 输入很重要：尽可能多的加入人为经验的高效特征项
    - 架构：设计多个蓄水池网络，不同的网络具备不同的谱半径，对于长记忆的隔k步更新，用于保持长记忆性；同时运行多次使特征项更丰富。加入脉冲化-后续过一个sigmoid（与AWQ相反，权重不量化，信号量化成01）。

1. 使用AI在这种PINN场景中需要注意的
   1. 硬件资源有限，乘法器数量大幅受限，无法使用现代基于Transformer的模型架构，难以落地
   2. 由于最终要落地ASIC，以及计算资源的限制，像是LSTM这种模型由于无法实行流水线化，在工业界难以落地，但效果很好
   3. 非线性函数不能使用relu这种，过于低效

---
## DPD vs. Deepseek

### Deepseek的了解
1. 最新的模型是v3和o1模型。v3作为通用大预言模型参数量671B，激活37B。训练token为14.8T。
2. 模型架构主要有以下3点
   1. 基于transformer架构
   2. 最核心的是引入了MoE混合专家模型
   大幅的提升了模型规模
   负载均衡：1. 固定每个expert的token输入 2. 随机路由增加探索 3. 使用auxiliary loss，最小化分配比例的平方和
   3. MTP多令牌预测
   解决decoder自回归一次解码只生成一个token效率低的问题，解码加速；比如Speculative Decoding（投机解码）采用了两个大小模型，小模型预测，大模型验证的思路，加速推理

3. 模型训练
   1. 混合精度训练
   前向用fp8，回传用fp32
   2. 知识蒸馏和CoT chain of thought
   3. RL强化学习
   GRPO，PPO


自己部署：用ollama部署了7B小模型；

### 谈谈对大模型大看法
- 技术理解
  - 基础架构
    - Transformer核心：自注意力机制（Self-Attention）实现长程依赖建模，相比RNN/CNN更适合处理序列数据。
    - 规模定律（Scaling Laws）：模型性能随参数量、数据量、计算量幂律提升，但需平衡边际效益（如Chinchilla最优数据比例）。
    - 关键技术：
    位置编码：RoPE（旋转位置编码）缓解长文本位置偏差。
    高效训练：3D并行（数据/张量/流水线）、混合精度训练（FP16+BF16）。
    推理优化：KV缓存、动态批处理（如vLLM）、量化（GPTQ/AWQ）。
  - 模型家族
  通用基座模型：GPT-4、Claude 3、LLaMA-3，deepseekV3强调多任务泛化能力。
  垂直领域模型：代码：DeepSeek-Coder；多模态：Gemini 1.5、Sora（视频生成）。
- 行业落地
  - 生产力工具：Copilot（代码/写作辅助）、ChatPDF（文档交互）。
  - 企业服务：客服自动化（如Cohere Command R+）、智能数据分析（如Tableau集成LLM）。
- 关键挑战
  - 算力垄断：依赖英伟达H100/A100，国产芯片（如华为昇腾）仍在追赶。
  - 安全与对齐：幻觉（Hallucination）
  - 长尾场景：小样本适应能力不足，需结合RAG（检索增强生成）或微调。
  - 数据瓶颈：高质量数据逐渐耗尽，合成数据（如LLM自生成）与课程学习是关键
- 未来趋势
  短期：模型小型化（1B-10B参数）是落地重点。


---
# 深度学习
- 如果提高模型泛化能力
1. 数据增强 2. 正则化 3. dropout 4. 更改网络结构
-  BN、LN、IN、GN和SN的区别
![alt text](image-214.png)
- CNN演化史
  AlexNet为历史突破，以下为几个方向
  - 网络逐渐加深：VGG16-VGG19 -
  - 增强卷积模块功能： NIN-GoogleNet-Inception
  - 从分类到检测任务：R-CNN-Fast R-CNN-Faster R-CNN
  - 增加新的单元：Inception V2，FCN，STNet
- 介绍一下VGG/inception/resnet
  - VGG的思想就是加深网络，减小卷积核尺寸3*3
  - inception模块的思想一是通过1*1的卷积层降低维度和计算量2是通过不同大小的卷积核提取不同感受野的特征并进行拼接![alt text](image-215.png) 
  - resnet通过学习残差，来构建深度更深的神经网络，达到更
好的学习效果（resnet32，resnet50， resnet152）
- 描述googlenet发展的几个过程
  - inceptionv1提出了1*1的卷积核和多尺度的卷积核
  - v2加入了BN层
  - v3提出了卷积分解，将n*n的卷积核变为1*n和n*1.加速计算，提升了网络深度和非线性
  - v4结合了inception模块和resnet，继续加深网络
- 简述bilinear CNN的工作流程（目标检测）
  BilinearCNN，该模型中会有两个并列的网络A和B(均由卷积层、池化层等构成)，两个网络在后边会通过外积合并到一起得到bilinearvector，最后再通过softmax层。工作流程是：给模型喂进去输入图像后，网络A的作用是对物体部件进行定位，即完成物体与局部区域检测工作，而网络B则是用来对网络A检测到的物体位置进行特征提取。两个网络相互协调作用，完成了细粒度图像分类过程中两个最重要的任务:物体、局部区域的检测与特征提取。

---
# 大模型面经
- layer norm和rms norm
  - layer norm
  ![alt text](image-216.png)
  - rms norm
  ![alt text](image-217.png)
  ![alt text](image-218.png)
- attention
  - 存在的问题
  - 变体
    - 稀疏attention
- 介绍一下deepseek
  V3模型：通用大语言模型和R1模型：推理专用
  - 模型架构
    - 基于标准的Decoder-only架构
    - GQA分组注意力机制
    - 稀疏混合专家（MoE）设计：
      路由算法使用Top-K门控（如K=2），并引入负载均衡损失（如Switch Transformer的辅助损失），防止专家负载不均。
  - 训练方法
  有监督->强化学习（PPO）
  - 推理部署
    量化部署：AWQ（Activation-aware Quantization）：保护重要权重通道的精度，减少量化误差。
    KV缓存优化：PagedAttention（类似vLLM）：将KV缓存分页管理，支持灵活的内存分配，提升并发推理能力。
  - 性能表现