0. 自我介绍
面试官您好，我叫唐锐。本科毕业于北京交通大学，专业是通信工程，硕士毕业于荷兰代尔夫特理工大学，专业是信号与系统。研究方向为凸优化，深度学习和分布式优化。硕士论文课题为分布式粒子滤波，实现多传感器场景下的目标追踪。提出了基于高斯过程的重采样和基于启发式算法的传感器融合算法，大幅降低通信开销，相比sota降低了8倍。目前就职于华为，工作时长为2年半。主要的工作大致分为两类，一是使用AI对通信非线性系统进行建模，问题可以抽象为时序信号回归问题。包括使用CNN设计特征提取层与当前的回归模型相结合，使得相关通信指标提升3dB，和高校和研究所合作，从头设计LSTM,Transformer及ESN架构的模型。创新的提出基于长短期的esn架构并引入snn脉冲化大幅降低计算量。
第二部分主要是加速模型训练和超参寻优HPO相关的工作。模型训练加速采用koopman算法对网络参数的收敛路径进行预测，最终将训练时长缩短到原来的1/2-1/3。超参寻优采用了基于网格搜索和SH，基本能保证每种设计的模型在经过寻优后性能提升约1dB。



1. 介绍一下是怎么使用CNN提取特征的，完成了一个什么样的任务？
- 项目背景：简单介绍一下DPD，属于动态系统建模（针对PA系统状态建模），使用行为建模（对输入输出关系建模）的方式解决,与神经网络非常契合。可以抽象成监督回归任务。现有的模型为一个多层的神经网络模型，模型有固定的若干个人为设定的特征（出于对复杂度的要求，使用先验知识提前设计了一些特征项）。好处是复杂度低，缺点是过度依赖先验，后续持续提升模型性能乏力。目的是设计一个高效的特征生成模块。
- 难点：深度学习有多种特征提取的方法，MLP，CNN，Transformer，包括PCA。难点在于以低复杂度设计一个特征提取模块，同时现有的模型为一个灰盒模型，特征提取模块的输出要具备物理意义，作为后级的输入。比如所有的节点的输出都应该在具体的频率附近。
- 为了尽量利用先验信息，采用了基于CNN的特征提取。如何设计输入的矩阵。最初在设计时两个维度，其中一个维度是不同时刻（包含过去和未来），另一个维度是不同的特征项，这边会引入一些先验的信息，对于复数的信号，引入了i,q,绝对值，绝对值的平方项。一个有效的特征项在时间维度上频率应该是平滑变化的，构造的特征项需要满足一定的条件，为原始信号*（基于原始信号绝对值的非线性函数）。还是借助激活函数来实现。当时就想到传统的基于多项式特征项的方法，tanh()函数在0附近做泰勒展开为一次，三次，五次，等奇次项的展开，很好的对应了多项式特征。当时试了很多不同的激活函数，例如relu，...,等，效果最好的是tanh函数。通过堆积卷积和激活函数的操作，在最后添加一个全连接层，得到最终不同特征项的i和q值，送入后级网络。后续尝试引入TCN的架构，采用1*1卷积+深度可分离卷积，将原先的二维卷积分解成1维卷积的级联。同时在深度可分离卷积中采用膨胀卷积，每层通过指数级增加膨胀卷积的间隔，可以通过较小的层数获取更长的记忆。同是500左右的参数量级，第二种方案相对于第一种在性能上提升了1~2dB。
![alt text](image-253.png)
- ModernTCN
1. 除此之外还有哪些常用的特征提取方法？
   1. 传统机器学习
      1. PCA（降维，通过正交转换将相关的变量转化为不相关的变量，同时保留方差较大的）
      2. FFT/小波变换，获取时序信号的频率特征（是否具有某种模式）
      3. 均值，方差这些统计量；cv领域会更多一些，相关的边缘检测算法，直方图信息...
   2. 深度学习
      1. CNN
      2. LSTM
      3. Attention
      4. Autoencoder

2. 你觉得CNN相比于LSTM, Transformer在DPD任务种的优势和劣势是什么
   1. 优势
      1. 计算效率高，相比于LSTM适合并行计算，同时没有反馈结构可以实现流水线计算。这对于实时性要求很高的DPD系统至关重要。
      2. CNN由于共享权重，参数量相较于Transformer等复杂结构更少，不容易过拟合
      3. Transformer L^2的复杂度，和QKV矩阵的引入，复杂度过高
   2. 缺点
      1. 感受野受限于卷积核的大小，难以建模长记忆效应。但对于DPD建模来说，不像长文本模型具备过长的记忆性，所以影响较小

3. 用了哪些lstm和transformer的模型？
   1. 因为lstm不具备流水线计算的能力，所有没有花精力研究，简单试了下多层lstm结构。两层1w5参数lstm效果就以及挺好了。lstm有三个门，遗忘门（决定此时哪些信息从记忆单元中遗忘），输出门（决定了下一个隐藏状态），输入门（决定此时哪些信息被存入记忆单元）。除了输出外，另外引入了记忆单元这个变量。记忆单元通过遗忘门连接至上一时刻，类似残差连接的效果，所以减弱了梯度消失。
   2. transformer主要试了几个变种：
      1. informer
       - 针对transformer在长序列信号处理时的O(L^2)复杂度，提出了ProbSparse Attention，对于每个q，只对log(L)个key进行计算，只取topu个q，其余的都做平均（attention score具有非常明显的长尾性）
       - 针对多层transformer占内存的行为，使用distill在序列方向降低长度，通常是 1D convolution + max pooling
       - 使用了生成式解码器（the generative style decoder），一次性生成所有的输出
      2. pyraformer
       - 通过金字塔结构
       - 将复杂度降低到O(L),最小路径距离降低到O(1)
   3. 有没有针对dpd场景作什么优化
      1. informer效果差：DPD属于“短窗口+高分辨率”任务，相比堆叠很深的encoder，浅层结构+大感受野往往更有效。同时probsparse
        仅保留encoder结构，且确保第 n 位置的 query 永远保留。

4. 有没有用过机器学习的方法，例如集成学习
集成学习的方法没用过，但是尝试用过MoE的架构，模型后级为8个模块相加，每个模块的结构一致，但是输入不同。当时考虑对于每个输入，不使用32个模块的输出，而是挑选最合适的31个。做过实验对于输入，在训练完的模型下，挑选最优的31个点。
第一种思路：带标签的数据分类问题，MLP, CNN，包括机器学习RF,GBDT, XGBOOST（需要指定max depth和num of estimators），数据不存在可分类性，分类精度最多只到70左右，GBDT这些过拟合会比较严重。
- xgboost
[如何调参](https://developer.baidu.com/article/details/3333266)
![alt text](image-264.png)

第二种思路：直接设计分类器，随同回归模型一起训练。
    - 如何确定模型结构（不能太复杂）
    - 模型的输入数据怎么选择
结论最后可以有1db的提升，但是分类器的复杂度太高了，不如直接在原始模型上增加资源。遂放弃。

对输入进行分类，不同的输入过不同的分类器，分类这步骤尝试使用了机器学习(流形学习)的方法，但是实验无法找到可分类性：
[link](https://www.bilibili.com/video/BV12HE8zsEnb/?spm_id_from=333.337.search-card.all.click&vd_source=d7339479a3c0de1eb46f840baa9a3510)
- pca：计算协方差矩阵，但是缺陷是只能是线性映射
- t-sne：核心为高维和低维映射后的结果具有相同的分布。高维中给定一个点以及其相邻的若干个点，通过引入高斯或其他概率分布函数将分布和距离建立起关系。同理在低维度每个点也应该有相同的分布，通过最小化KL散度实现求解。相邻的点的数量即通过perplexity来控制。缺点是计算复杂度高。
- umap：与tsne核心思想一致，但是其不是通过趋同两个分布函数，而是通过趋同两个图分布（使得邻接矩阵相似）。如何构造图即使用k-nearest neghbors实现。

1. 简单说一下时序任务和其他任务的不同？预测和异常检测的不同
时序数据的核心在于有明确的时间顺序，当前值依赖于过去，甚至是将来一段时间的上下文信息。建模目标为捕捉时序依赖，动态变化。
典型的时序任务包含预测，异常检测，时序分类。异常检测，时序分类都属于分类任务，所有不同任务的模型结构是一致的；
异常检测基于预测残差和重构误差来进行判断。

1. 如何避免过拟合
   1. 降低模型复杂度
   2. 加正则化
   3. 扩充数据集
   4. early stopping
   5. dropout

2. 用什么方法做特征选择
   1. 基于物理先验
   2. 基于贪婪算法OMP
3. 决策树是怎么工作的，选取基尼指数还是特征增益来进行选择？
![alt text](image-254.png)
基尼系数越小越好
随机森林的两个关键：1. 样本的有放回随机采样 2. 每棵树划分时，随机选取部分特征用于最优划分，而非全特征；减少了单棵数高方差的问题，随机特征避免了树的雷同
避免过拟合：预剪枝（提前控制叶子节点个数，深度），后剪枝：后序遍历，剪掉子树，查看效果
![alt text](image-255.png)
1. 为什么不使用机器学习，例如xgboost建模？
![alt text](1751778165582.png)

1.  说说求解混合整数线性规划有哪些算法?
所有求解的算法可以分为精确算法和启发算法两大类：
精确算法比较经典的有
    1. 分支定界法（适合小规模问题）：通过递归地分割问题的解空间，并在此过程中利用“定界”来剪枝无效分支
    优点：保证能找到最优解
    缺点：复杂度高，最坏情况下可能需要枚举所有整数
    分为DFS和BFS两种策略，现代求解器基于DFS，BFS为广度优先，占用内存大，例如深度为10，最多需要存储2^10个节点，而深度优先最多只需要存储21个节点。其次深度优先更容易找到可行解从而进行定界剪枝。
    2. 割平面法：通过逐步添加线性不等式（割平面）来切割松弛问题的可行域，逼近整数解。
    3. 两者进行结合加快求解，gurobi用的分支切割算法
   生成割平面-分支-定界（割平面无效时切换为分支法）
    4. 两者对比：
   通常分支定界法实现相对容易，更适合更一般的MILP问题，但缺点也比较明显，比较消耗内存。而割平面法空间占用较小，但割平面构造困难，选取不当会导致收敛慢。
启发式算法适合规模较大时，寻找较优解：
GA可用于MILP求解，核心思想在于仅对整形变量编码为染色体，同时对于每个染色体针对连续变量求解最优，根据目标值作为fitness进行选择-交叉-变异
一般变量数>1000，可采用启发式算法。

1.  求解问题的规模大概是多少？
![alt text](image-256.png)
- 支持异步，非可靠网络；通信复杂度更低
- 9/16-node；agent：5-50；变量维度为几十到几百。以向量形式呈现，每个agent有5个约束，总约束为agent*5.求解时间为几秒到几时秒。
![alt text](image-257.png)


12. 介绍一下有哪些cnn网络
图像任务：分类，检测，分割，跟踪

13. R-CNN的算法流程
    ![alt text](image-259.png)
    1.  生成候选区域（selective search）2000个
    2.  对于每个候选区域，使用深度网络（VGG16）提取特征；缩放到227*227（4096维）
    3.  特征送入每一类svm分类器，判断是否属于该类；（使用非极大值抑制）
    ![ ](image-258.png)
    4.  使用回归器修正位置
    缺点：
    5. 候选框重叠，特征提取重复计算，训练所需空间大
14. Fast R-CNN
    ![alt text](image-261.png)
   1. 生成候选区域（与R-CNN一致）（并非全部使用，随机采样一部分）
   2. 对整张图计算特征，并将SS生成的候选框投影到原始图中
   3. 每个ROI缩放到7*7，并通过MLP得到分类结果和位置结果（不再分成两个网络训练）
    损失函数：
    ![alt text](image-260.png)
15. Faster R-CNN
    ![alt text](image-262.png)
    1.  对整张图计算特征
    2.  使用RPN生成候选框，并投影到特征图上
    3.  每个ROI缩放到7*7，并通过MLP得到分类结果和位置结果
    损失函数：RPN loss和Fast R-CNN loss
    ![alt text](image-263.png)



16. 在校期间一些信号处理的项目（偏传统）（STAR）
**问题背景+目标+工作/动作+结果**
    1.  音频（语音增强）
    2.  通信（角度估计）（接收器）
    3.  图像Faster R-CNN


信号处理
17. ESN
🏠Situation:
通信系统中射频链路的最后一步是功率放大器，小信号最终会经过功率放大器发射出去，但是实际的功率放大器往往不是理想线性的，比如会有饱和区，以及其他一些模拟器件的热效应，外围电路等原因都会导致非线性。同时，功放的更高的效率往往意味着更强的非线性程度。如果我们不加以校正，发生畸变的信号在接收机处误码率就会大幅增高，同时非线性变换也会导致频谱泄露，即泄露到相邻信道，这也是3GPP协议明确禁止的。所以为了进行校正，我们在信号进入功率放大器前，会先让信号通过一个训练好的预失真模型进行补偿, 那么我们的目标就是让这个模型的的系统响应接近功放的系统响应的逆变换。

因为难以对功放系统进行建模，业界主流采用行为建模的方式，也就是给定预失真系统的输入和理想输出，对非线性进行建模。因为功放系统为一个带有记忆效应的非线性系统，传统以及当前工业界使用的主要有两者，一种为基于模型特征项的单层线性回归模型，核心思想就是提取最重要的特征（volterra级数可以提供足够的非线性阶次和记忆深度，理论上可以以任意精度逼近，多项式，在宽频系统中交调项，三次项，甚至是五次项）。优势就是实现简单，复杂度低，但缺点也很明显，就是效率低(参数过多，5阶和记忆长度为8时，参数量达到4320)，性能难以提升。第二种也就是级联模型, weiner模型（LTI-NL）/hammerstein模型(NL-LTI)/weiner-hammerstein模型（均为级联模型），在音频系统中也经常使用。选择weiner还是hammerstein取决于非线性是在输入端还是输出端。随着功放越复杂，通常已经无法满足。

像这种传统模型性能逐渐到瓶颈（491MHz），本质可以抽象为一个时间序列非线性预测或者回归问题。可以用一系列深度学习时序回归的模型去做，例如，RNN，LSTM，Transformer等等。使用神经网络进行建模的核心难点在于最终该算法在终端运行，对模型复杂度要求很高，同时工业应用对于精度要求也很高。

🏆Target：
因为ESN回声状态网络相比于其他神经网络天然具备记忆能力，回传资源低，训练速度快。我的工作是联合高校和俄研所，设计基于ESN结构的预失真模型，目标精度（nmse）控制在10^-5，非常低，复杂度控制在（乘法器数量<3000）。

✏️Action：
ESN属于RNN，由两个部分组成，蓄水池和输出层，其中蓄水池为一系列连接关系和权重随机生成后并固定的神经元。输入通过稀疏的输入矩阵和蓄水池中的神经元进行连接。核心思想是将低维的输入投影到高维的特征状态空间，最后经过一个线性的输出层得到输出。在ESN中，蓄水池层模拟了一个动态系统，且只有线性输出层需要训练，训练复杂度极低，大幅降低了神经网络在误差回传阶段的资源。
1. 我首先完成了ESN的复现并用于数字预失真建模，在这一步通过选择网络的输入特征，调整网络参数和规模提升模型性能。输入特征这一块主要是使用了前期相关工作的一些经验知识，除了IQ信号之外，信号的模，相位信息，双频信号则会再加入交调项去丰富模型的输入；网络参数调整主要有3个方面，第一个就是权重矩阵的稀疏性，第二个是ESP（因为蓄水池是模拟的动态系统的行为，这个参数用来保证动态系统的稳定，防止发散），第三个是leaky rate（泄露率），控制系统状态对于对于输入的敏感性。最后将网络规模提升到10000，稀疏性0.1可以实现目标精度，此时意味着10000*10000的乘法矩阵，即使稀疏性为0.1复杂度也无法接受。
2. 为了提升模型能力，我分别从两个点切入，由于功放是一个带有记忆效应的非线性系统，分别提升1.模型的非线性程度2.模型的记忆深度。非线性程度我在原有模型上进行了两部分改动，第一采用级联的结构，实验中发现从1层提升到4层，性能提升明显，持续加深收益不大。第二个部分则是在级联的蓄水池之间引入固定的非线性，实验发现在层与层之间引入多次项和交叉项，性能提升明显。这与传统模型的先验信息相符，同时可以降低蓄水池的规模。如果原始N^2+2N，通过这个操作可以降低为N，（降低N+2）倍。至于模型的记忆深度，由于ESN不存在LSTM类似的门控机制，所以很难同时获得长时记忆和短时记忆。
- ESN
    - 提出背景：1. 传统 RNN 训练困难（梯度爆炸/消失）2. 全参数更新意味着时间复杂度高，训练时间长
    - 创新：把动态记忆能力（循环网络）固定住，只训练最后的输出层
    - 优势：训练快：只训练输出层；动态建模能力强：Reservoir 提供丰富的动态表示避免梯度问题：不用 BPTT；实现简单：适合硬件部署（如 FPGA）；直接得到闭式解：
  ![alt text](image-231.png)
    若需要在线更新，可使用RLS算法：
    ![alt text](image-140.png)
    - 局限:Reservoir 随机，需调参（如谱半径、稀疏度）；只能处理中短期依赖（长依赖处理不如 LSTM）；任务结构不明确时，Reservoir 可能效率不高
    - 初始化reservoir：谱半径（矩阵的最大特征值模）<1(采用后归一法)；保证Echo State Property（ESP），在足够长时间后，Reservoir 的状态只依赖于输入序列，而不依赖于初始状态。
    ![alt text](image-138.png)
    - 输入很重要：尽可能多的加入人为经验的高效特征项
  
    - 架构：设计多个蓄水池网络，不同的网络具备不同的谱半径，对于长记忆的隔k步更新，用于保持长记忆性；同时运行多次使特征项更丰富。加入脉冲化-后续过一个sigmoid（与AWQ相反，权重不量化，信号量化成01）。

1. 级联，并联
2. 
模型复杂度分为两块：前向和解算。ESN属于RNN循环网络的分支，存在以下几个显著的优势：1. 不存在RNN中梯度消失的问题 2. 解算复杂度极低（训练很快）3. 天然带有记忆效应；被常用于动态系统建模，预测，分类任务。
1. leaky-integrated/non-spiking/discrete/continuous value
2. ridge regression(offline) & force learning(online)
3. echo state property 
4. 如何提高记忆容量
   1. 增大储备池大小
   2. 增大谱半径
   3. 合适的输入缩放（过大的缩放会导致神经元饱和，历史信息被淹没）
   4. 稀疏连接：稀疏性降低了储备池的有效耦合强度，使状态演化更平缓，信息衰减更慢（类似“回声”持续时间更长）。实验表明，稀疏储备池的记忆容量更接近理论上限 
5. 缺陷：
   1. 固定的随意连接限制了性能
   2. 单纯的提高reservoir大小无法持续提升性能
6. 训练方式
   1. ridge regression
   2. online LMS
7. 不同的结构
   1. deepESN：多层结构引入（1）不同时间尺度的信息-越前级的ESN由于直接处理原始输入信号，包含高频成分，更适合捕捉短期依赖（2）不同频率的信息，递归的结构天然具备低通滤波器的特性，后级接收的是前一层已处理过的、抽象化的信号，高频成分被逐步滤除。
8. 为什么效果会好，理论上可以从以下两个方面阐述：
   1. short-term memory capacity如何变化
   2. richness of reservoir states如何变化（Uncoupled Dynamics），主要针对多reservoir的情况
🎊results
提出一种级联结构模型，有效融合了LSTM的长短期依赖捕捉能力；并创新性地引入积分点火模型 (Integrate-and-Fire) 实现神经元间信息的脉冲化传递 (Spiking Neural Network, SNN)。该架构在保持模型表征能力的同时，大幅降低了模型复杂度与计算开销。

关于传感器数据，训练数据的获取：使用cyclic方法，搭建完整的射频链路，发数器-上变频-LNA-功放（反馈链路）（增益控制）

物理特性：
- 由于物理器件的原因，非线性带有“记忆效应”带来建模难度，在低功率区具有很好的线性度
- 仅关注频点附近的失真情况,带外的通过滤波器滤除，可适当降低建模难度，因此在构建建模项人为需要关注阶数，避免做很多带外无谓的建模，浪费资源
- 宽频时：与单频的区别，由于频谱宽，非线性维度变高，模型项变多（交调-跨频段，三次项）；相位响应更复杂，需要花更多资源进行建模
  
1. 传统的方式：volterra级数、
2. 模型规模要小 1. 制程限制，资源限制2. 功耗限制，芯片功耗低，主要功耗集中在功放 3. 模型参数实时更新，并非纯推理
3. 未来的研究方向：4G-5G-6G，不可避免的带宽越来越大，PA效率越高，DPD的复杂度会越来越高；另一方面，MIMO天线阵列，对单射频模块需要多个功放，每个功放都需要配一个模型，通过更改为复用一个较大规模的DPD,然后不同的功放额外增加小模型的方式去降低复杂度。


📗ESN在信号处理上的应用：
1. 时间序列预测-气象信号预测（温度，风速，降雨量）
2. 生物医学信号（脑电，心电等非平稳信号）
3. 适用于边缘设备上的轻量级信号处理
📍机械设备故障诊断中的振动传感器信号预测
工业设备（如电机、齿轮箱）运行过程中会产生振动。正常与故障状态下的振动信号表现出不同的时序模式：
- 正常运行时，信号相对稳定但可能存在周期性变化；
- 故障发生前，信号可能出现非平稳变化（如能量增强、频率突变）；
- 故障发生时，信号会迅速突变，表现出强非线性和不规则性。
🚫 异常检测任务，采用重构误差作为代理指标（没有真实标签）：
模型尝试复现“正常”状态数据；当预测误差大于某个阈值，就认为可能发生异常
⭐为什么适合ESN?
- 动态响应快	能及时响应振动信号变化
- 高维非线性映射	能捕捉复杂的动态模式（如由轻微磨损到突发断裂的过程）
- 状态跟踪能力强	能追踪故障演化趋势（非平稳过程）
- 只训练输出层	训练速度快，适合工业在线系统


🏫 在校项目
1️⃣ 语音增强去噪算法（往往非平稳）
基于频谱减法、维纳滤波(线性最优)、MMSE算法（基于后验概率的，引入语音和噪声的先验信息）进行信号增强
估计噪声的功率谱密度:
- MS（minumum statistics最小统计量）噪声估计，无需语音活动检测
  - bartlette功率谱估计
  - 在一个滑动窗口内，对每个频点取最小值
- MMSE（引入语音存在概率（Speech Presence Probability, SPP）以提高对低信噪比（SNR）或非平稳噪声环境下的鲁棒性）
2️⃣ 通信天线阵列入射角DOA估计
- MVDR（最小方差无失真响应）
  基于优化的，目标为最小化阵列输出的总功率，同时确保在信号方向上无失真，压低噪声。因为涉及协方差矩阵求逆，当两个信号角度相近时，矩阵求逆会不稳定。
- MUSIC
  X=AS+N, 通过对E(XX^H)特征值分解，特征值大的特征向量组合张成信号空间，特征值小的特征向量组合张成噪声空间，两者完全正交。通过遍历不同入射角的导向向量与噪声空间相乘并取倒数，波峰极为对应入射角。

