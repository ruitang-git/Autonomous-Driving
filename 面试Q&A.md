0. 自我介绍
面试官您好，我叫唐锐。本科毕业于北京交通大学，专业是通信工程，硕士毕业于荷兰代尔夫特理工大学，专业是信号与系统。研究方向为信号处理，深度学习和分布式优化。在校期间做过像是基于频谱减法，维纳滤波的语音增强算法、包括基于MVDR（最小方差无失真响应）, MUSIC的入射角DOA估计，深度学习方向也做过基于faster r-cnn的图像识别。我硕士论文的研究内容为分布式粒子滤波，实现多传感器场景下的目标追踪。我提出了基于高斯过程的重采样和基于启发式算法的传感器融合算法，大幅降低通信开销，相比sota降低了8倍。目前就职于华为，工作时长为2年半左右。主要的工作大致分为两块，一是使用AI对通信中射频链路的信号进行处理，保证整个链路的输入和最终输出逼近理想的线性放大，因为这个问题可以抽象为时序信号的预测回归问题，可以借助AI算法对传统算法进行改良。我的工作包括使用CNN设计特征提取层与当前的回归模型相结合，使得相关通信指标提升3dB。同时也负责研究基于纯神经网络的模型研究，比如基于lstm，transformer。在这一方面，我将ESN回声状态网络应用到该建模场景中，创新的提出基于长短期记忆的级联架构大幅提升模型性能，并在此基础上引入神经元脉冲化进一步降低资源开销。第二部分主要是加速模型训练和超参寻优HPO相关的工作。模型训练加速采用koopman算法对网络参数的收敛路径进行预测，最终将训练时长缩短到原来的1/2-1/3。超参寻优采用了基于网格搜索和SH，基本能保证每种设计的模型在经过寻优后性能提升约1dB。以上是我的自我介绍。

🏫 在校项目
1️⃣ 语音增强去噪算法（往往非平稳）
基于频谱减法、维纳滤波(线性最优)、MMSE算法（基于后验概率的，引入语音和噪声的先验信息）进行信号增强
估计噪声的功率谱密度:
- MS（minumum statistics最小统计量）噪声估计，无需语音活动检测
  - bartlette功率谱估计
  - 在一个滑动窗口内，对每个频点取最小值
- MMSE（引入语音存在概率（Speech Presence Probability, SPP）以提高对低信噪比（SNR）或非平稳噪声环境下的鲁棒性）
2️⃣ 通信天线阵列入射角DOA估计
- MVDR（最小方差无失真响应）
  基于优化的，目标为最小化阵列输出的总功率，同时确保在信号方向上无失真，压低噪声。因为涉及协方差矩阵求逆，当两个信号角度相近时，矩阵求逆会不稳定。
- MUSIC
  X=AS+N, 通过对E(XX^H)特征值分解，特征值大的特征向量组合张成信号空间，特征值小的特征向量组合张成噪声空间，两者完全正交。通过遍历不同入射角的导向向量与噪声空间相乘并取倒数，波峰极为对应入射角。


1. CNN
🏆Target：
当前中射频链路的数字预失真模型近似一个多层的神经网络模型，但是当前模型的输入为提前选取好的固定特征。好处是复杂度低，模型效率高。缺点是过度依赖先验，后续持续提升模型性能乏力。因此工作目标是设计一个高效的特征生成模块。

🎊results
基于CNN设计了特征提取层，并通过引入TCN架构和膨胀卷积进一步降低复杂度。通过将特征提取层与当前模型相级联，性能提升3dB，参数额外增加约500.

🏠Situation:
通信系统中射频链路的最后一步是功率放大器，小信号最终会经过功率放大器发射出去，但是实际的功率放大器往往不是理想线性的，比如会有饱和区，以及其他一些模拟器件的热效应，外围电路等原因都会导致非线性。同时，功放的更高的效率往往意味着更强的非线性程度。如果我们不加以校正，发生畸变的信号在接收机处误码率就会大幅增高，同时非线性变换也会导致频谱泄露，即泄露到相邻信道，这也是3GPP协议明确禁止的。所以为了进行校正，我们在信号进入功率放大器前，会先让信号通过一个训练好的预失真模型进行补偿, 那么我们的目标就是让这个模型的的系统响应接近功放的系统响应的逆变换。

因为功放系统为一个带有记忆效应的非线性系统，理论上可以使用基于模型特征项的单层线性回归模型，特征来源为volterra级数（多项式），可以提供足够的非线性阶次和记忆深度。出于对模型复杂度的要求，会从volterra级数中挑选高效的特征项，主要是利用正交匹配追踪（OMP）。但随着模型逐渐加深，OMP无法工作。

✏️Action：
深度学习有多种特征提取的方法，MLP，CNN，Transformer，包括PCA。难点在于以低复杂度设计一个特征提取模块，同时现有的模型为一个灰盒模型，特征提取模块的输出因为要作为当前模型的输入，因此必须服从一些物理约束。因为信号为复数信号，那么就要求特征应该在具体的频率附近，时序信号的相位和频率不应该有突变，应该是平滑的。因此为了尽量利用先验信息，采用了基于CNN的特征提取。我首先设计了输入张量。因为预失真模型是一个非线性且带有记忆效应的系统，我将输入设置为二维矩阵，其中一个维度是不同时刻（包含过去和未来），另一个维度是不同的特征项，这边会引入一些先验的信息，对于复数的信号，引入了i,q,绝对值，绝对值的平方项，特征项维度的选择原则为volterra技术的高效基函数。但此时的输出为矩阵的线性变化，无法满足非线性和此前提到的物理约束。在我们的任务中，激活函数除了提供非线性，还承担了使输出在物理层面上合法的工作。通过将激活函数进行泰勒展开，可以得到每层的输出表达式。最终敲定使用tanh()函数作为激活函数，原因在于在0附近对tanh函数做泰勒展开正好为一次，三次，五次，等奇次项的展开，很好的对应了volterra多项式特征。实验也表明众多激活函数例如relu，sigmoid，selu，elu等等，效果最好的是tanh函数，不同的激活函数性能差距很大（5dB）。最后通过堆积卷积和激活函数的操作，在最后添加一个全连接层，得到最终不同特征项的i和q值，送入后级网络。
为了进一步提升性能，降低复杂度。下一步我引入了TCN（时序卷积网络）的架构，采用1*1卷积+深度可分离卷积，将原先的二维卷积分解成1维卷积的级联。同时在深度可分离卷积中采用膨胀卷积，每层通过指数级增加膨胀卷积的间隔，可以通过较小的层数获取更长的记忆。同是500左右的参数量级，第二种方案相对于第一种在性能上提升了1~2dB。通过设计了上述的特征提取层，并与当前模型相级联，最终性能成功提升3dB。



1. 除此之外还有哪些常用的特征提取方法？
   1. 传统机器学习
      1. PCA（降维，通过正交转换将相关的变量转化为不相关的变量，同时保留方差较大的）
      2. FFT/小波变换，获取时序信号的频率特征（是否具有某种模式）
      3. 均值，方差这些统计量；cv领域会更多一些，相关的边缘检测算法，直方图信息...
   2. 深度学习
      1. CNN
      2. LSTM
      3. Attention
      4. Autoencoder

2. 你觉得CNN相比于LSTM, Transformer在DPD任务种的优势和劣势是什么
   1. 优势
      1. 计算效率高，相比于LSTM适合并行计算，同时没有反馈结构可以实现流水线计算。这对于实时性要求很高的DPD系统至关重要。
      2. CNN由于共享权重，参数量相较于Transformer等复杂结构更少，不容易过拟合
      3. Transformer L^2的复杂度，和QKV矩阵的引入，复杂度过高
   2. 缺点
      1. 感受野受限于卷积核的大小，难以建模长记忆效应。但对于DPD建模来说，不像长文本模型具备过长的记忆性，所以影响较小

3. 用了哪些lstm和transformer的模型？
   1. 因为lstm不具备流水线计算的能力，所有没有花精力研究，简单试了下多层lstm结构。两层1w5参数lstm效果就以及挺好了。lstm有三个门，遗忘门（决定此时哪些信息从记忆单元中遗忘），输出门（决定了下一个隐藏状态），输入门（决定此时哪些信息被存入记忆单元）。除了输出外，另外引入了记忆单元这个变量。记忆单元通过遗忘门连接至上一时刻，类似残差连接的效果，所以减弱了梯度消失。
   2. transformer主要试了几个变种：
      1. informer
       - 针对transformer在长序列信号处理时的O(L^2)复杂度，提出了ProbSparse Attention，对于每个q，只对log(L)个key进行计算，只取topu个q，其余的都做平均（attention score具有非常明显的长尾性）
       - 针对多层transformer占内存的行为，使用distill在序列方向降低长度，通常是 1D convolution + max pooling
       - 使用了生成式解码器（the generative style decoder），一次性生成所有的输出
      2. pyraformer
       - 通过金字塔结构
       - 将复杂度降低到O(L),最小路径距离降低到O(1)
   3. 有没有针对dpd场景作什么优化
      1. informer效果差：DPD属于“短窗口+高分辨率”任务，相比堆叠很深的encoder，浅层结构+大感受野往往更有效。同时probsparse
        仅保留encoder结构，且确保第 n 位置的 query 永远保留。

4. 有没有用过机器学习的方法，例如集成学习
集成学习的方法没用过，但是尝试用过MoE的架构，模型后级为8个模块相加，每个模块的结构一致，但是输入不同。当时考虑对于每个输入，不使用32个模块的输出，而是挑选最合适的31个。做过实验对于输入，在训练完的模型下，挑选最优的31个点。
第一种思路：带标签的数据分类问题，MLP, CNN，包括机器学习RF,GBDT, XGBOOST（需要指定max depth和num of estimators），数据不存在可分类性，分类精度最多只到70左右，GBDT这些过拟合会比较严重。
- xgboost
[如何调参](https://developer.baidu.com/article/details/3333266)
![alt text](image-264.png)

第二种思路：直接设计分类器，随同回归模型一起训练。
    - 如何确定模型结构（不能太复杂）
    - 模型的输入数据怎么选择
结论最后可以有1db的提升，但是分类器的复杂度太高了，不如直接在原始模型上增加资源。遂放弃。

对输入进行分类，不同的输入过不同的分类器，分类这步骤尝试使用了机器学习(流形学习)的方法，但是实验无法找到可分类性：
[link](https://www.bilibili.com/video/BV12HE8zsEnb/?spm_id_from=333.337.search-card.all.click&vd_source=d7339479a3c0de1eb46f840baa9a3510)
- pca：计算协方差矩阵，但是缺陷是只能是线性映射
- t-sne：核心为高维和低维映射后的结果具有相同的分布。高维中给定一个点以及其相邻的若干个点，通过引入高斯或其他概率分布函数将分布和距离建立起关系。同理在低维度每个点也应该有相同的分布，通过最小化KL散度实现求解。相邻的点的数量即通过perplexity来控制。缺点是计算复杂度高。
- umap：与tsne核心思想一致，但是其不是通过趋同两个分布函数，而是通过趋同两个图分布（使得邻接矩阵相似）。如何构造图即使用k-nearest neghbors实现。

1. 简单说一下时序任务和其他任务的不同？预测和异常检测的不同
时序数据的核心在于有明确的时间顺序，当前值依赖于过去，甚至是将来一段时间的上下文信息。建模目标为捕捉时序依赖，动态变化。
典型的时序任务包含预测，异常检测，时序分类。异常检测，时序分类都属于分类任务，所有不同任务的模型结构是一致的；
异常检测基于预测残差和重构误差来进行判断。

1. 如何避免过拟合
   1. 降低模型复杂度
   2. 加正则化
   3. 扩充数据集
   4. early stopping
   5. dropout

2. 用什么方法做特征选择
   1. 基于物理先验
   2. 基于贪婪算法OMP
3. 决策树是怎么工作的，选取基尼指数还是特征增益来进行选择？
![alt text](image-254.png)
基尼系数越小越好
随机森林的两个关键：1. 样本的有放回随机采样 2. 每棵树划分时，随机选取部分特征用于最优划分，而非全特征；减少了单棵数高方差的问题，随机特征避免了树的雷同
避免过拟合：预剪枝（提前控制叶子节点个数，深度），后剪枝：后序遍历，剪掉子树，查看效果
![alt text](image-255.png)
1. 为什么不使用机器学习，例如xgboost建模？
![alt text](1751778165582.png)

1.  说说求解混合整数线性规划有哪些算法?
所有求解的算法可以分为精确算法和启发算法两大类：
精确算法比较经典的有
    1. 分支定界法（适合小规模问题）：通过递归地分割问题的解空间，并在此过程中利用“定界”来剪枝无效分支
    优点：保证能找到最优解
    缺点：复杂度高，最坏情况下可能需要枚举所有整数
    分为DFS和BFS两种策略，现代求解器基于DFS，BFS为广度优先，占用内存大，例如深度为10，最多需要存储2^10个节点，而深度优先最多只需要存储21个节点。其次深度优先更容易找到可行解从而进行定界剪枝。
    2. 割平面法：通过逐步添加线性不等式（割平面）来切割松弛问题的可行域，逼近整数解。
    3. 两者进行结合加快求解，gurobi用的分支切割算法
   生成割平面-分支-定界（割平面无效时切换为分支法）
    4. 两者对比：
   通常分支定界法实现相对容易，更适合更一般的MILP问题，但缺点也比较明显，比较消耗内存。而割平面法空间占用较小，但割平面构造困难，选取不当会导致收敛慢。
启发式算法适合规模较大时，寻找较优解：
GA可用于MILP求解，核心思想在于仅对整形变量编码为染色体，同时对于每个染色体针对连续变量求解最优，根据目标值作为fitness进行选择-交叉-变异
一般变量数>1000，可采用启发式算法。

1.  求解问题的规模大概是多少？
![alt text](image-256.png)
- 支持异步，非可靠网络；通信复杂度更低
- 9/16-node；agent：5-50；变量维度为几十到几百。以向量形式呈现，每个agent有5个约束，总约束为agent*5.求解时间为几秒到几时秒。
![alt text](image-257.png)


12. 介绍一下有哪些cnn网络
图像任务：分类，检测，分割，跟踪

13. R-CNN的算法流程
    ![alt text](image-259.png)
    1.  生成候选区域（selective search）2000个
    2.  对于每个候选区域，使用深度网络（VGG16）提取特征；缩放到227*227（4096维）
    3.  特征送入每一类svm分类器，判断是否属于该类；（使用非极大值抑制）
    ![ ](image-258.png)
    4.  使用回归器修正位置
    缺点：
    5. 候选框重叠，特征提取重复计算，训练所需空间大
14. Fast R-CNN
    ![alt text](image-261.png)
   1. 生成候选区域（与R-CNN一致）（并非全部使用，随机采样一部分）
   2. 对整张图计算特征，并将SS生成的候选框投影到原始图中
   3. 每个ROI缩放到7*7，并通过MLP得到分类结果和位置结果（不再分成两个网络训练）
    损失函数：
    ![alt text](image-260.png)
15. Faster R-CNN
    ![alt text](image-262.png)
    1.  对整张图计算特征
    2.  使用RPN生成候选框，并投影到特征图上
    3.  每个ROI缩放到7*7，并通过MLP得到分类结果和位置结果
    损失函数：RPN loss和Fast R-CNN loss
    ![alt text](image-263.png)


17. ESN
🏆Target：
针对中射频链路的数字预失真模块，设计基于ESN回声状态网络的预失真模型，目标精度（nmse）控制在10^-5。

🎊results
因为ESN回声状态网络相比于其他神经网络具有的一个明显的优势是回传资源低，训练速度快，适合通信这种实时性要求高，复杂度低的场景。我设计了一种级联结构模型，有效将LSTM的长短期依赖捕捉能力融入进ESN，大幅提高了模型的非线性建模能力和记忆深度；并引入了神经元的脉冲化，进一步降低了模型的资源开销。该架构能够满足目标精度，同时模型参数量相比于基础ESN模型，降低近80倍。

🏠Situation:
通信系统中射频链路的最后一步是功率放大器，小信号最终会经过功率放大器发射出去，但是实际的功率放大器往往不是理想线性的，比如会有饱和区，以及其他一些模拟器件的热效应，外围电路等原因都会导致非线性。同时，功放的更高的效率往往意味着更强的非线性程度。如果我们不加以校正，发生畸变的信号在接收机处误码率就会大幅增高，同时非线性变换也会导致频谱泄露，即泄露到相邻信道，这也是3GPP协议明确禁止的。所以为了进行校正，我们在信号进入功率放大器前，会先让信号通过一个训练好的预失真模型进行补偿, 那么我们的目标就是让这个模型的的系统响应接近功放的系统响应的逆变换。

因为难以对功放系统进行建模，业界主流采用行为建模的方式，也就是给定预失真系统的输入和理想输出，对非线性进行建模。因为功放系统为一个带有记忆效应的非线性系统，传统以及当前工业界使用的主要有两者，一种为基于模型特征项的单层线性回归模型，核心思想就是提取最重要的特征（volterra级数可以提供足够的非线性阶次和记忆深度，理论上可以以任意精度逼近，多项式，在宽频系统中交调项，三次项，甚至是五次项）。优势就是实现简单，复杂度低，但缺点也很明显，就是效率低(参数过多，5阶和记忆长度为8时，参数量达到4320)，性能难以提升。第二种也就是级联模型, weiner模型（LTI-NL）/hammerstein模型(NL-LTI)/weiner-hammerstein模型（均为级联模型），在音频系统中也经常使用。选择weiner还是hammerstein取决于非线性是在输入端还是输出端。随着功放越复杂，通常已经无法满足。

像这种传统模型性能逐渐到瓶颈（491MHz），本质可以抽象为一个时间序列非线性预测或者回归问题。可以用一系列深度学习时序回归的模型去做，例如，RNN，LSTM，Transformer等等。使用神经网络进行建模的核心难点在于最终该算法在终端运行，对模型复杂度要求很高，同时工业应用对于精度要求也很高。

✏️Action：
ESN属于RNN，由两个部分组成，蓄水池和输出层，其中蓄水池为一系列连接关系和权重随机生成后并固定的神经元。输入通过稀疏的输入矩阵和蓄水池中的神经元进行连接。核心思想是将低维的输入投影到高维的特征状态空间，最后经过一个线性的输出层得到输出。在ESN中，蓄水池层模拟了一个动态系统，且只有线性输出层需要训练，训练复杂度极低，大幅降低了神经网络在误差回传阶段的资源。
1. 我首先完成了ESN的复现并用于数字预失真建模，在这一步通过选择网络的输入特征，调整网络参数和规模提升模型性能。输入特征这一块主要是使用了前期相关工作的一些经验知识，除了IQ信号之外，信号的模，相位信息，双频信号则会再加入交调项去丰富模型的输入；网络参数调整主要有3个方面，第一个就是权重矩阵的稀疏性，第二个是ESP（因为蓄水池是模拟的动态系统的行为，这个参数用来保证动态系统的稳定，防止发散），第三个是leaky rate（泄露率），控制系统状态对于对于输入的敏感性。最后将网络规模提升到10000，稀疏性0.1可以实现目标精度，此时意味着10000*10000的乘法矩阵，即使稀疏性为0.1复杂度也无法接受。
2. 为了提升模型能力，我分别从两个点切入，由于功放是一个带有记忆效应的非线性系统，分别提升1.模型的非线性程度2.模型的记忆深度。非线性程度我在原有模型上进行了两部分改动，第一采用级联的结构，实验中发现从1层提升到4层，性能提升明显，持续加深收益不大。第二个部分则是在级联的蓄水池之间引入固定的非线性，实验发现在层与层之间引入多次项和交叉项，性能提升明显。这与传统模型的先验信息相符，同时可以降低蓄水池的规模。如果原始N^2+2N，通过这个操作可以降低为N，（降低N+2）倍。关于模型的记忆深度，由于ESN不存在LSTM类似的门控机制，所以很难同时获得长时记忆和短时记忆。我在模型中设计了三条并行的链路，分别对应于长记忆，中记忆，短记忆。长记忆链路，采用隔k时间步更新的策略，可以获取更长时间尺度的信息。短记忆采用两个手段去除历史输入的影响，1是每隔k步初始化蓄水池系数，2是通过控制更新矩阵的系数，避免模型受初始系数影响过大。总结一下，通过分别从非线性和记忆深度入手更改模型结构，相比原始参数下降40倍。
3. 为继续降低复杂度，我引入了脉冲升级网络中的脉冲化。其实ESN，LSM（液体状态机），通常和类脑神经网络一起研究。通过在模型后级引入积分点火模型，将神经元输出01化（神经元受到相邻神经元的信号加权，若高于设定阈值则激发1，否则输出0），此时乘法均变为加法，资源大幅减小。最终模型等效参数量在28000，mse达到目标精度1e-5.



- 初始化reservoir：谱半径（矩阵的最大特征值模）<1(采用后归一法)；保证Echo State Property（ESP），在足够长时间后，Reservoir 的状态只依赖于输入序列，而不依赖于初始状态。
![alt text](image-138.png)
- 输入很重要：尽可能多的加入人为经验的高效特征项
- 架构：设计多个蓄水池网络，不同的网络具备不同的谱半径，对于长记忆的隔k步更新，用于保持长记忆性；同时运行多次使特征项更丰富。加入脉冲化-后续过一个sigmoid（与AWQ相反，权重不量化，信号量化成01）。

- 为什么效果会好，理论上可以从以下两个方面阐述：
   1. short-term memory capacity如何变化
   2. richness of reservoir states如何变化（Uncoupled Dynamics），主要针对多reservoir的情况
- 如何提高记忆容量
   1. 增大储备池大小
   2. 增大谱半径
   3. 合适的输入缩放（过大的缩放会导致神经元饱和，历史信息被淹没）
   4. 稀疏连接：稀疏性降低了储备池的有效耦合强度，使状态演化更平缓，信息衰减更慢（类似“回声”持续时间更长）。实验表明，稀疏储备池的记忆容量更接近理论上限 
- 缺陷：
   1. 固定的随意连接限制了性能
   2. 单纯的提高reservoir大小无法持续提升性能
- 不同的结构
   1. deepESN：多层结构引入（1）不同时间尺度的信息-越前级的ESN由于直接处理原始输入信号，包含高频成分，更适合捕捉短期依赖（2）不同频率的信息，递归的结构天然具备低通滤波器的特性，后级接收的是前一层已处理过的、抽象化的信号，高频成分被逐步滤除。



关于传感器数据，训练数据的获取：使用cyclic方法，搭建完整的射频链路，发数器-上变频-LNA-功放（反馈链路）（增益控制）

物理特性：
- 由于物理器件的原因，非线性带有“记忆效应”带来建模难度，在低功率区具有很好的线性度
- 仅关注频点附近的失真情况,带外的通过滤波器滤除，可适当降低建模难度，因此在构建建模项人为需要关注阶数，避免做很多带外无谓的建模，浪费资源
- 宽频时：与单频的区别，由于频谱宽，非线性维度变高，模型项变多（交调-跨频段，三次项）；相位响应更复杂，需要花更多资源进行建模
  
1. 传统的方式：volterra级数、
2. 模型规模要小 1. 制程限制，资源限制2. 功耗限制，芯片功耗低，主要功耗集中在功放 3. 模型参数实时更新，并非纯推理
3. 未来的研究方向：4G-5G-6G，不可避免的带宽越来越大，PA效率越高，DPD的复杂度会越来越高；另一方面，MIMO天线阵列，对单射频模块需要多个功放，每个功放都需要配一个模型，通过更改为复用一个较大规模的DPD,然后不同的功放额外增加小模型的方式去降低复杂度。


📗ESN在信号处理上的应用：
1. 时间序列预测-气象信号预测（温度，风速，降雨量）
2. 生物医学信号（脑电，心电等非平稳信号）
3. 适用于边缘设备上的轻量级信号处理
📍机械设备故障诊断中的振动传感器信号预测
工业设备（如电机、齿轮箱）运行过程中会产生振动。正常与故障状态下的振动信号表现出不同的时序模式：
- 正常运行时，信号相对稳定但可能存在周期性变化；
- 故障发生前，信号可能出现非平稳变化（如能量增强、频率突变）；
- 故障发生时，信号会迅速突变，表现出强非线性和不规则性。
🚫 异常检测任务，采用重构误差作为代理指标（没有真实标签）：
模型尝试复现“正常”状态数据；当预测误差大于某个阈值，就认为可能发生异常
⭐为什么适合ESN?
- 动态响应快	能及时响应振动信号变化
- 高维非线性映射	能捕捉复杂的动态模式（如由轻微磨损到突发断裂的过程）
- 状态跟踪能力强	能追踪故障演化趋势（非平稳过程）
- 只训练输出层	训练速度快，适合工业在线系统




